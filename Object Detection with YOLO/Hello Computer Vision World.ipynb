{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Fundamentals\n",
    "\n",
    "This notebook was a precursor to the project on object detection with YOLO. This was for me to ease into the computer vision problem and also try my hand at what's been called the \"Hello World\" of neural networks and computer vision. <br>\n",
    "\n",
    "The goal is simple - correctly identify digits from a dataset of tens of thousands of handwritten images. In essence, this is a multi-class classification problem. Before diving into neural networks and seeing how they perform, let's benchmark a few other models like logistic regression and Support Vector Machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read in the data from sklearn datasets\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data has 70,000 digits. We will use the first 60,000 as training data and the rest as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 784)\n",
      "Testing data shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data basically has 28X28 images of digits from 0 to 9. Each of these pixels is one column in our data, which is why the 784 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to shuffle the images before we train the models since it helps performance on some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(60000)\n",
    "\n",
    "X_train = X_train[shuffled_indices]\n",
    "y_train = y_train[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing that we can do is to scale the inputs using sklearn `StandardScaler`. This helps models using SGD to converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_test_scaled = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for model fitting. Our approach with each model will be as follows - <br>\n",
    "1. Fit the model on the training data.\n",
    "2. Get the accuracy on training data.\n",
    "3. Get cross validation accuracy on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1:** *Logistic Regression* <br>\n",
    "Logistic regression is a binary classifier which means that it is built for classification problems with two classes. To use it for multi-class classification, there are two strategies - <br>\n",
    "1. **One vs Rest Classifier** - This means that we build 'k' logistic classifiers (k being the number of classes in the target). Then we use each of these classifiers to predict the output class and use the class with the highest probability score.\n",
    "2. **One vs One classifier** - Here we build nC2 classifiers and then we train these classifiers on the two classes only. This requires much less data.\n",
    "3. **Softmax Regression** (Multinomial logistic regression) - This involves representing the target values as vectors and then using the softmax function for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train a **One vs Rest logistic regression** classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# here we specify the OneVsRest classifier strategy\n",
    "log_reg = LogisticRegression(multi_class = \"ovr\", solver = \"liblinear\")\n",
    "\n",
    "# sklearn by default fits the OneVsRest classifier for logistic regression\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# store the output for later use\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(log_reg, \"log_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "log_reg = joblib.load(\"log_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93095"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_pred = log_reg.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg_scores = cross_val_score(log_reg,\n",
    "                                X_train_scaled,\n",
    "                                y_train,\n",
    "                                cv = 3,\n",
    "                                scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.9104830889013348\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross validation score:\", np.mean(log_reg_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the logistic regression model performs fairly well - A 93% accuracy on the training set and 91% cross validation accuracy suggest that it is performing almost equally well on unseen data as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2:** - *Multinomial logistic regression model - softmax!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "smax_clf = LogisticRegression(multi_class = \"multinomial\", solver = 'lbfgs', max_iter = 1000)\n",
    "smax_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9442"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "y_train_pred = smax_clf.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation accuracy\n",
    "smax_clf_scores = cross_val_score(smax_clf,\n",
    "                                 X_train_scaled,\n",
    "                                 y_train,\n",
    "                                 cv = 3,\n",
    "                                 scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.91\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross validation score: {:.2f}\".format(np.mean(smax_clf_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the softmax model does better on the training data, we see that it slightly underperforms on unseen data (as indicated by the cross validation scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 3:** *Random Forest Classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# train accuracy\n",
    "y_train_pred = rf_clf.predict(X_train_scaled)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# cross validation accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_clf_scores = cross_val_score(rf_clf,\n",
    "                                X_train_scaled,\n",
    "                                y_train,\n",
    "                                cv = 3,\n",
    "                                scoring = \"accuracy\")\n",
    "\n",
    "print(\"CV accuracy: {:.2f}\".format(np.mean(rf_clf_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest achieves a 100% accuracy on the training data but it is clearly overfitting since cross validation accuracy is just 94%. This is still better than the models that came before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far, I have applied some of the more fundamental models to the problem of identifying digits in the MNIST data. Let's try fitting a neural network using `keras` and see how it performs. A basic neural network model has the following components - <br>\n",
    "1. Input data\n",
    "2. Layers - These take the input data and extract representations relevant to the problem at hand.\n",
    "3. Loss function - Calculates how far the output produced is from the actual outputs\n",
    "4. Optimizer - Takes the loss function and translates that into changes in the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1:** - Fully connected neural network <br>\n",
    "For the problem at hand, lets build a neural network with just one hidden layer with 16 neurons and an output layer with 10 neurons - one for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "X_train_nn = X_train_scaled[:50000]\n",
    "X_val_nn = X_train_scaled[50000:]\n",
    "\n",
    "y_train_nn = y_train[:50000]\n",
    "y_val_nn= y_train[50000:]\n",
    "\n",
    "network1 = models.Sequential()\n",
    "network1.add(layers.Dense(16, activation = 'relu', input_shape = (784,)))\n",
    "network1.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer\n",
    "For problems with multi-class classification, we generally use categorical cross entropy as the loss function. We will be using the `rmsprop` variant of the gradient descent to fine tune the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'rmsprop',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Now that we have the basic elements set up, lets just go ahead and train the network on our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.5249 - acc: 0.8527 - val_loss: 0.2925 - val_acc: 0.9177\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2652 - acc: 0.9251 - val_loss: 0.2546 - val_acc: 0.9306\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.2279 - acc: 0.9364 - val_loss: 0.2388 - val_acc: 0.9351\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2071 - acc: 0.9413 - val_loss: 0.2313 - val_acc: 0.9348\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.1926 - acc: 0.9459 - val_loss: 0.2278 - val_acc: 0.9358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d788cc2be0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat = to_categorical(y_train_nn)\n",
    "y_val_cat = to_categorical(y_val_nn)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "network1.fit(X_train_nn, \n",
    "            y_train_cat, \n",
    "            batch_size = 128, \n",
    "            epochs = 5,\n",
    "           validation_data = (X_val_nn, y_val_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2:** - Convolutional Neural Network <br>\n",
    "A convolutional neural network, in contrast to the fully connected neural network, has quite a few number of parameters to train. This is also different from the basic neural network in the fact that while we passed a flattened matrix as input above, we will be sending each image as a 4D tensor (of the form (instances, height, width, channels)) into the CNN. \n",
    "\n",
    "CNNs are tuned to image classification tasks and this should significantly outperform the above neural network.\n",
    "\n",
    "Let's see what kind of accuracies we can get using a CNN on the same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 887,530\n",
      "Trainable params: 887,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# reshape - this is important for CNN since it takes its inputs as a height X width array and not a row array\n",
    "X_train_cnn = X_train_scaled[:50000].reshape(-1, 28, 28, 1)\n",
    "X_val_cnn = X_train_scaled[50000:].reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test_scaled.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# since we will model a softmax output with 10 neurons, we need output with this!\n",
    "y_train_cnn = to_categorical(y_train[:50000], num_classes = 10)\n",
    "y_val_cnn = to_categorical(y_train[50000:], num_classes = 10)\n",
    "\n",
    "# initialize a sequential network\n",
    "network = models.Sequential()\n",
    "\n",
    "# add a convolution layer with 32 filters with f = 5, and input image of dimensions 28X28x1\n",
    "network.add(layers.Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                          activation ='relu', input_shape = (28,28,1)))\n",
    "\n",
    "# add another convolution layer with 32 filters and input that of the previous layer\n",
    "network.add(layers.Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu'))\n",
    "\n",
    "# add a maxpool layer with f =2, s = 1\n",
    "network.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# add a dropout parameter that drops 25% of weights to 0 randomly. This is sort of like regularization for neural networks.\n",
    "network.add(layers.Dropout(0.25))\n",
    "\n",
    "# add another convolution layer with 64 filters with f = 3 and input that of the previous layer\n",
    "network.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "\n",
    "# add another convolution layer with 64 filters with f = 3 and input that of the previous layer\n",
    "network.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "\n",
    "# add maxpool layer with f = 2, s = 2\n",
    "network.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# add dropout to drop 25% of weights to 0 randomly\n",
    "network.add(layers.Dropout(0.25))\n",
    "\n",
    "# after the 2 conv layers, maxpool, another conv layers and maxpool, we need to flatten the output\n",
    "network.add(layers.Flatten())\n",
    "\n",
    "# add a normal neural network with 256 units\n",
    "network.add(layers.Dense(256, activation = \"relu\"))\n",
    "\n",
    "# another dropout\n",
    "network.add(layers.Dropout(0.5))\n",
    "\n",
    "# final layer with 10 way output since we have 10 classes\n",
    "network.add(layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network structure is created, now we setup loss function, optimizer and a metric to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      " - 467s - loss: 0.2076 - acc: 0.9361 - val_loss: 0.0520 - val_acc: 0.9841\n",
      "Epoch 2/3\n",
      " - 475s - loss: 0.0715 - acc: 0.9793 - val_loss: 0.0442 - val_acc: 0.9873\n",
      "Epoch 3/3\n",
      " - 474s - loss: 0.0561 - acc: 0.9843 - val_loss: 0.0465 - val_acc: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "\n",
    "# Compile the model\n",
    "network.compile(optimizer = optimizer , \n",
    "                loss = \"categorical_crossentropy\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the network\n",
    "history = network.fit(X_train_cnn,\n",
    "                      y_train_cnn,\n",
    "                      batch_size = 80,\n",
    "                      epochs = 3, \n",
    "                      verbose = 2,\n",
    "                      validation_data = (X_val_cnn, y_val_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the CNN model gives us a validation accuracy of 98.5% which is 4% higher than the neural network with fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the test data\n",
    "\n",
    "Let's test these models on the test data now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.9171\n",
      "Softmax Regression: 0.9212\n",
      "Random Forest: 0.9474\n",
      "10000/10000 [==============================] - 0s 35us/step\n",
      "Neural Network (Fully Connected): 0.9428\n",
      "10000/10000 [==============================] - 32s 3ms/step\n",
      "CNN: [0.03470213618227135, 0.9897]\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression:\", accuracy_score(y_test, log_reg.predict(X_test_scaled)))\n",
    "print(\"Softmax Regression:\", accuracy_score(y_test, smax_clf.predict(X_test_scaled)))\n",
    "print(\"Random Forest:\", accuracy_score(y_test, rf_clf.predict(X_test_scaled)))\n",
    "print(\"Neural Network (Fully Connected):\", network1.evaluate(X_test_scaled, y_test_cat)[1])\n",
    "print(\"CNN:\", network.evaluate(X_test_scaled.reshape(-1, 28, 28, 1), y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the CNN outperforms all the other models by a significant margin. I can improve the model still with other techniques such as data augmentation and learning schedules!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
